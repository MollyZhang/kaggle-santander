4-26-2016
1.  original number of features: 371,
    0-variance features: 34,
    duplicate features: 29,
    linearly covariate features: 21
    remove all, now we have 287 features

using XGBoost, learning curve:
sample train       val
1000   0.936551  0.768611
6000   0.875586  0.827448
11000  0.863039  0.824784
16000  0.858613  0.823298
21000  0.854142  0.828317
26000  0.851990  0.830302
31000  0.849438  0.829749
36000  0.847078  0.829305
41000  0.845280  0.828891
46000  0.842998  0.829130
51000  0.842585  0.829975
56000  0.843980  0.830870


4-27-2016
# try standard scaling before xgboost, no improvement (note standard scaler performs a lot better than MinMax scaler)


4-28
remove a lot more features with almost no variance or highly correlated with others
now total number of features 247
learning curve now, interesly, with less sample, the classifier performs better
          train       val
1000   0.949341  0.802144
6000   0.895919  0.821986
11000  0.870245  0.827978
16000  0.863032  0.830104
21000  0.861706  0.829614
26000  0.858253  0.831848
31000  0.852160  0.829561
36000  0.850164  0.831624
41000  0.847568  0.829951
46000  0.845912  0.829931
51000  0.843732  0.831374
56000  0.842595  0.831304




